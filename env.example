# Aline Knowledge Importer Configuration
# Copy this file to .env and customize for your environment

# ===== MINIMUM REQUIRED CONFIGURATION =====
# Choose ONE of these Firecrawl options:

# Option 1: Local Firecrawl (recommended - no API key needed)
USE_LOCAL_FIRECRAWL=true
LOCAL_FIRECRAWL_URL=http://localhost:3002

# Option 2: Cloud Firecrawl (requires API key from firecrawl.dev)
# USE_LOCAL_FIRECRAWL=false
# FIRECRAWL_API_KEY=fc-your_firecrawl_api_key_here

# Output directory for processed files
OUTPUT_DIR=./output

# Default team ID for CLI operations
DEFAULT_TEAM_ID=aline123

# ===== THAT'S IT! Everything below is optional =====

# ===== Database Configuration =====
# SQLite database path for job queue (NO Redis required!)
DATABASE_PATH=./data/queue.db

# ===== Optional Server Configuration =====
# Port for the API server (if running REST API)
PORT=3001

# Host interface to bind to
HOST=0.0.0.0

# Number of concurrent workers for processing
NUM_WORKERS_PER_QUEUE=4

# ===== Optional Firecrawl Settings =====
# For Firecrawl cloud service (only if using cloud option)
FIRECRAWL_API_URL=https://api.firecrawl.dev

# ===== Content Processing Configuration =====
# Maximum words per content chunk
MAX_CHUNK_SIZE=1000

# Minimum words per content chunk
MIN_CHUNK_SIZE=100

# Overlap size between chunks (words)
CHUNK_OVERLAP_SIZE=50

# Whether to split content on markdown headers (true/false)
SPLIT_ON_HEADERS=true

# ===== Crawler Behavior =====
# Maximum pages to crawl per domain
MAX_PAGES_PER_DOMAIN=100

# Maximum crawl depth from root URL
MAX_CRAWL_DEPTH=3

# Delay between requests in milliseconds (be respectful!)
REQUEST_DELAY=1000

# User agent string for web requests
USER_AGENT=Aline-Knowledge-Importer/1.0

# ===== AI-Powered Features (Optional) =====
# Gemini API key for AI summarization and tagging
GEMINI_API_KEY=your_gemini_api_key_here

# Enable AI-powered document summarization
ENABLE_AI_SUMMARIZATION=false

# Enable AI-powered content tagging
ENABLE_AI_TAGGING=false

# ===== Cloud Storage (Optional) =====
# Enable automatic upload to S3
ENABLE_S3_UPLOAD=false

# AWS S3 configuration (if enabled)
AWS_REGION=us-east-1
AWS_S3_BUCKET=aline-knowledge-base
AWS_ACCESS_KEY_ID=your_aws_access_key
AWS_SECRET_ACCESS_KEY=your_aws_secret_key

# ===== Logging Configuration =====
# Log level: error, warn, info, debug
LOG_LEVEL=info

# Log file path
LOG_FILE=./logs/importer.log

# ===== Development & Debug =====
# Enable debug mode for extra logging
DEBUG=false

# Enable verbose output
VERBOSE=false

# Enable development mode features
NODE_ENV=production

# ===== Security & Rate Limiting =====
# API rate limiting (requests per minute)
RATE_LIMIT_PER_MINUTE=60

# Maximum file size for PDF processing (bytes)
MAX_PDF_SIZE_BYTES=104857600

# Request timeout in milliseconds
REQUEST_TIMEOUT_MS=30000

# ===== Advanced Features =====
# Enable distributed crawling (requires Redis - future enhancement)
ENABLE_DISTRIBUTED_CRAWLING=false

# Redis URL for distributed queue management (NOT REQUIRED for basic operation)
REDIS_URL=redis://localhost:6379

# Enable content deduplication
ENABLE_DEDUPLICATION=true

# Content similarity threshold for deduplication (0-1)
DEDUPLICATION_THRESHOLD=0.85

# Enable content validation
ENABLE_CONTENT_VALIDATION=true

# Minimum content quality score (0-1)
MIN_CONTENT_QUALITY=0.5

# ===== Webhook Configuration (Optional) =====
# Webhook URL to notify when processing completes
WEBHOOK_URL=

# Webhook secret for verification
WEBHOOK_SECRET=

# ===== Monitoring & Analytics =====
# Enable performance metrics collection
ENABLE_METRICS=false

# Metrics export endpoint
METRICS_ENDPOINT=http://localhost:9090/metrics

# Health check endpoint path
HEALTH_CHECK_PATH=/health

# ===== Example Team Configurations =====
# Default team ID for CLI operations
DEFAULT_TEAM_ID=aline-default

# Team-specific output directories (comma-separated)
# Format: team_id:directory,team_id:directory
TEAM_OUTPUT_DIRS=aline123:./output/aline123,team2:./output/team2

# ===== Integration Settings =====
# Slack webhook for notifications (optional)
SLACK_WEBHOOK_URL=

# Discord webhook for notifications (optional)
DISCORD_WEBHOOK_URL=

# Email notification settings (optional)
SMTP_HOST=
SMTP_PORT=587
SMTP_USER=
SMTP_PASS=
NOTIFICATION_EMAIL=

# ===== Performance Tuning =====
# Memory limit for PDF processing (MB)
PDF_MEMORY_LIMIT_MB=512

# Concurrent PDF processing limit
MAX_CONCURRENT_PDF=2

# Network timeout for web requests (ms)
NETWORK_TIMEOUT_MS=30000

# Retry attempts for failed requests
MAX_RETRY_ATTEMPTS=3

# Exponential backoff base delay (ms)
RETRY_BASE_DELAY_MS=1000

# ===== Experimental Features =====
# Enable OCR for image-heavy PDFs (requires Tesseract)
ENABLE_OCR=false

# Enable content translation (requires Google Translate API)
ENABLE_TRANSLATION=false
TARGET_LANGUAGE=en

# Enable content sentiment analysis
ENABLE_SENTIMENT_ANALYSIS=false

# ===== Quick Start Examples =====
# For local development with Firecrawl:
# USE_LOCAL_FIRECRAWL=true
# LOCAL_FIRECRAWL_URL=http://localhost:3002
# LOG_LEVEL=debug
# VERBOSE=true

# For production with cloud Firecrawl:
# USE_LOCAL_FIRECRAWL=false
# FIRECRAWL_API_KEY=fc-your-key-here
# LOG_LEVEL=info
# ENABLE_S3_UPLOAD=true

# For high-volume processing:
# NUM_WORKERS_PER_QUEUE=8
# MAX_CHUNK_SIZE=1500
# ENABLE_DISTRIBUTED_CRAWLING=true
# REDIS_URL=redis://your-redis-cluster:6379 